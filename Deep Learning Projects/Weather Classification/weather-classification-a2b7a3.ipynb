{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1> Motivation: </h1>\n<h3> Student for <i>IIT - Delhi</i> proposed using Computer Vision to predict the AQI (Air Quality Index). They this ideology to be easily accessable, hence proposed creating an Android App using TF Lite. They called this project as <span color='red'>Celestini Project</span></h3>","metadata":{}},{"cell_type":"code","source":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('sBTqLHA95rs', width=1700, height=800)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Required Libraries","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function, division\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport numpy as np\nimport plotly.express as px\nimport time\nimport os\n# from collections import Counter\n# import copy\nimport itertools\nimport glob\nfrom PIL import Image\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nimport torchvision\n# from torchvision.datasets import ImageFolder\nfrom torchvision import models, transforms\n# from torchvision.utils import make_grid\n\nfrom torch.utils.data import DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n# from torch.optim import lr_scheduler\n# from torch.autograd import Variable\nimport torch\nimport gc\nimport datetime\n\n\n# Setting Manual Seed for Recreation of results\ntorch.manual_seed(42)\nnp.random.seed(0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%bash\n# External Packages\n# pip -q install --upgrade pip\n# pip uninstall -y pillow\n# pip -q install pillow-simd\n# pip -q install torchsummary torch-lr-finder\n\n# from torchsummary import summary\n# from torch_lr_finder import LRFinder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def GetImagesFromFolder(PATH,Class_Folder,ext):\n    \"\"\"\n    This module can be used to import image data. It also takes care of resizing the images to 240x240 Pixels\n    \"\"\"\n    images = [Image.open(file).convert('RGB').resize((240,240),resample=Image.LANCZOS) for e in ext for file in glob.glob(PATH+Class_Folder+'/*.' + e)] \n    print(f\"Found {len(images)} in folder {Class_Folder}\")\n    np.random.shuffle(images)\n    return images,np.array([Class_Folder for i in range(len(images))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading Image Data","metadata":{}},{"cell_type":"code","source":"DATA_PATH = '/kaggle/input/multiclass-weather-dataset/Multi-class Weather Dataset/'\nFOLDERS = os.listdir(DATA_PATH)\next = ['jpg','jpeg']\nle = LabelEncoder().fit(FOLDERS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = datetime.datetime.now().replace(microsecond=0)\n\nALL_IMAGES,ALL_LABELS = [],[]\nimages_population ={}\n\nfor Class_Folder in FOLDERS:\n    IMAGES,LABELS = GetImagesFromFolder(DATA_PATH,Class_Folder,ext)\n    images_population[Class_Folder] = LABELS.shape[0]\n    ALL_IMAGES.extend(IMAGES)\n    ALL_LABELS.extend(LABELS)\n    \nend_time = datetime.datetime.now().replace(microsecond=0)\n\nprint(end_time - start_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Population Distribution of Data:\n* This is import to know if we have Imbalanced Dataset.\n* Luckily we have a consistent dataset","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame.from_dict({'Name':[i for i in images_population.keys()],'#Images':[i for i in images_population.values()]})\n\ncolors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen']\nfig = px.pie(df,values='#Images',names='Name', hole=.3, width=600, height=600,opacity=.80)\nfig.update_traces(hoverinfo='label+percent',textfont_size=20, marker=dict(colors=colors, line=dict(color='#000000', width=2)))\nfig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide', title={'text': 'Population of Various Classes','y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'})\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting Dataset to differernt splits for Training, Tesing & Validation:-","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nTRAIN_IMAGES, X_val_test, TRAIN_LABELS, y_val_test = train_test_split(ALL_IMAGES, ALL_LABELS, test_size=0.20, random_state=42,stratify=ALL_LABELS)\nVAL_IMAGES, TEST_IMAGES, VAL_LABELS, TEST_LABELS =  train_test_split(X_val_test, y_val_test, test_size=0.50, random_state=42,stratify=y_val_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Custom Dataset:","metadata":{}},{"cell_type":"code","source":"class WeatherDataset(Dataset):\n    def __init__(self, ImageData, Target, transform=None):\n        self.ImageData = ImageData\n        self.Target = torch.LongTensor(le.transform(Target))\n        self.transform = transform\n\n    def __getitem__(self, index):\n        x = self.ImageData[index]\n        y = self.Target[index]\n        if self.transform:\n            x = Image.fromarray(np.uint8(np.array(self.ImageData[index]))) # Memory Efficient way\n            x = self.transform(x)\n        return x, y\n    def __len__(self):\n        return len(self.ImageData)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This is a utility function to find out MEAN & STD for Normalizing Training Data","metadata":{}},{"cell_type":"code","source":"# SOURCE: https://youtu.be/y6IEcEBRZks\ndef get_mean_std(loader):\n    # VAR[X] = E[X**2] - E[X]**2\n    channels_sum, channels_squared_sum, num_batches = 0,0,0\n    \n    for data,_ in loader:\n        channels_sum +=torch.mean(data,dim=[0,2,3])\n        channels_squared_sum += torch.mean(data**2, dim=[0,2,3])\n        num_batches += 1\n    \n    mean = channels_sum/num_batches\n    std = (channels_squared_sum/num_batches - mean**2)**0.5\n    return mean,std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 25\ntransform = transforms.Compose([transforms.Resize((230,230)),transforms.ToTensor()])\ndataset = WeatherDataset(TRAIN_IMAGES, TRAIN_LABELS, transform=transform)\nloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=4)\nmean,std = get_mean_std(loader)\nprint(f\"Data loader has:\\n*\\tmean= {mean.tolist()}\\n*\\tstd= {std.tolist()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loader to Mini Batches:\nThis includes:-\n* Image Augementations\n* Mini Batch creation","metadata":{}},{"cell_type":"code","source":"transform = {'train':transforms.Compose([transforms.ColorJitter(),\n                                         transforms.RandomRotation(30),\n                                         transforms.Resize((240,240)),\n                                         transforms.RandomResizedCrop(230),\n                                         transforms.RandomHorizontalFlip(),\n                                         transforms.ToTensor(),\n                                         transforms.Normalize(mean=mean,std=std),#transforms.RandomErasing()\n                                        ]),\n             'val':transforms.Compose([transforms.Resize((230,230)),\n                                      transforms.ToTensor()]),\n             \n             'test':transforms.Compose([transforms.Resize((230,230)),\n                                      transforms.ToTensor()])}\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\nbatch_size = {'train':25, 'val':50,'test':50}\n\ndataset_classes = ['Cloudy','Rain','Shine','Sunrise']\n\n\nimage_datasets = {'train': WeatherDataset(TRAIN_IMAGES, TRAIN_LABELS, transform=transform['train']),\n                  'val':   WeatherDataset(VAL_IMAGES, VAL_LABELS, transform=transform['val']),\n                  'test':  WeatherDataset(TEST_IMAGES, TEST_LABELS, transform=transform['test'])\n}\n\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}\n\ndataloaders = {indx: torch.utils.data.DataLoader(image_datasets[indx], batch_size=batch_size[indx], num_workers=4, pin_memory=True, shuffle=True)\n              for indx in batch_size.keys()}\n\nprint(\"Size for Dataset:\\n\\t* Train: %d\\n\\t* Valid: %d\\n\\t* Test: %d\"%(dataset_sizes['train'],dataset_sizes['val'],dataset_sizes['test']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization of Training & Validation Data:\nPlease note, we are not visualizing Testing data, as it will be used for predictions.","metadata":{}},{"cell_type":"code","source":"# Helper function to display the image\ndef imshow(img):\n    # Convert from tensor image\n    plt.imshow(np.transpose(img, (1,2,0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one batch of training images\ndataiter = iter(dataloaders['train'])\nimages, labels = dataiter.next()\n# Convert images to numpy for display\nimages = images.numpy()\n\n# Plot the images in the batch\nfig = plt.figure(figsize=(25, 4))\n\nlabels = le.inverse_transform([i.item() for i in labels])\n\n# Display 20 images\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n    imshow(images[idx])\n    ax.set_title(labels[idx])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one batch of validation images\ndataiter = iter(dataloaders['val'])\nimages, labels = dataiter.next()\n# Convert images to numpy for display\nimages = images.numpy()\n\n# Plot the images in the batch\nfig = plt.figure(figsize=(25, 4))\n\n# Display 20 images\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n    imshow(images[idx])\n    ax.set_title(dataset_classes[labels[idx]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility Functions for Model Training & Visualization","metadata":{}},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, model_checkpoint=0, early_stop = 10, num_epochs=5):\n    start_time = datetime.datetime.now().replace(microsecond=0)\n    model = model.to(device)\n\n    # number of epochs to train the model\n    valid_loss_min = np.Inf # track change in validation loss\n    early_stop_cnt = 0\n    last_epoch_loss = np.Inf\n    globaliter = 0\n\n    final_loss = np.Inf\n\n    for epoch in range(1, num_epochs+1):\n        globaliter+=1\n        # keep track of training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n\n\n        ###################\n        # train the model #\n        ###################\n        model.train()\n        train_corrects = 0\n\n        for data, target in dataloaders['train']:\n            data, target = data.to(device), target.to(device)\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            _, preds = torch.max(output, 1)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update training loss\n            train_loss += loss.item()*data.size(0)\n            train_corrects += torch.sum(preds == target.data)\n\n        train_loss = train_loss/len(dataloaders['train'].dataset)\n        train_acc = (train_corrects.double()*100)/len(dataloaders['train'].dataset)\n\n        ######################    \n        # validate the model #\n        ######################\n        model.eval()\n        val_corrects = 0\n        for data, target in dataloaders['val']:\n            data, target = data.to(device), target.to(device)\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            _, preds = torch.max(output, 1)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # update average validation loss\n            valid_loss += loss.item()*data.size(0)\n            val_corrects += torch.sum(preds == target.data)\n\n        # calculate average losses\n        valid_loss = valid_loss/len(dataloaders['val'].dataset)\n        valid_acc = (val_corrects.double()*100)/len(dataloaders['val'].dataset)\n\n        # print training/validation statistics \n        print('Epoch: {} \\tTraining Loss:  {:.6f} \\tValidation Loss:  {:.6f}'.format(epoch, train_loss, valid_loss))\n        print('\\t\\tTraining Acc:  {:.3f} \\t\\tValidation Acc:  {:.3f}'.format(train_acc, valid_acc))\n\n        # save model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print('\\t\\tValidation loss decreased ({:.6f} --> {:.6f}).'.format(valid_loss_min,valid_loss))\n            if model_checkpoint != 0:\n                torch.save(model.state_dict(), '/kaggle/working/model.pt'.format(train_acc, valid_acc))\n                print('Model Saved: /kaggle/working/model.pt'.format(train_acc, valid_acc))\n            valid_loss_min = valid_loss\n        elif valid_loss == np.nan:\n            print(\"Model Loss: NAN\")\n\n        if (last_epoch_loss < valid_loss) and last_epoch_loss != np.Inf:\n            early_stop_cnt +=1\n            if early_stop_cnt == early_stop:\n                print('-'*50+\"\\nEarly Stopping Hit\\n\"+'-'*50)\n                break\n            else:\n                print('-'*50+f\"\\n\\t\\tEarly Stopping Step: {early_stop_cnt}/{early_stop}\\n\"+'-'*50)\n        else:\n            early_stop_cnt = 0\n            last_epoch_loss = valid_loss\n\n    print(f\"Training Completed with best model having loss of {round(valid_loss_min,6)}\")\n    del data,target\n    gc.collect()\n    end_time = datetime.datetime.now().replace(microsecond=0)\n    print(f'Time Taken: {end_time-start_time}')\n    return model\n\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n    size = len(classes)*2\n    plt.figure(figsize = (size,size))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title,fontsize=20)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes,fontsize=12)\n    plt.yticks(tick_marks, classes,fontsize=12)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label',fontsize=16)\n    plt.xlabel('Predicted label',fontsize=16)\n\n    \ndef model_verification(loader,batch_size,model,n_classes=5):\n    classes = list(le.inverse_transform([i for i in range(n_classes)]))\n    prediction_list,label_list = [],[]\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            predicted = outputs.argmax(dim=1).detach()\n            prediction_list.extend(predicted.tolist())\n            label_list.extend(labels.tolist())\n            \n    cm = confusion_matrix(prediction_list,label_list)\n    plot_confusion_matrix(cm, classes)\n    if device.type == 'cuda':\n        inputs = inputs.cpu()\n        labels = labels.cpu()\n    gc.collect()\n\n\ndef visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure(figsize = (num_images,num_images))\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['test']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'Predicted: {dataset_classes[preds[j]]} | Actual: {dataset_classes[labels[j]]}')\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)\n    if device.type == 'cuda':\n        inputs = inputs.cpu()\n        labels = labels.cpu()\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training:\nWe will be using DenseNet-161\n<center>\n    \n![DenseNet-161](https://mohuaxiao.github.io/2018/06/09/tensorflow/img/2018-6-9/densenet_Archi.JPG)\n</center>","metadata":{}},{"cell_type":"code","source":"n_classes = 4\nepochs = 25\n\nn_classes = len(dataset_classes)\n\nmodel_ft = models.densenet161(pretrained=True)\n# Using Model as Feature Extractor\nfor param in model_ft.parameters():\n    param.requires_grad = False\n\nnum_ftrs = model_ft.classifier.in_features\nmodel_ft.classifier = nn.Linear(num_ftrs, n_classes)\nmodel_ft = model_ft.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross Entropy Loss \ncriterion = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlr = 0.01\nmomentum = 0.5\ndecay = 0.01\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum = momentum, weight_decay = decay)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training of Model:')\nmodel_ft = train_model(model_ft, criterion, optimizer_ft,model_checkpoint=0,num_epochs=epochs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets visually check for the performance of our Model:\n1. Using Actual Vs Predicted\n2. Using Confusion Matrix","metadata":{}},{"cell_type":"code","source":"visualize_model(model_ft, num_images=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_verification(dataloaders['val'],batch_size['val'],model_ft,n_classes=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finally, lets see the performance on Testing Data:","metadata":{}},{"cell_type":"code","source":"model_verification(dataloaders['test'],batch_size['test'],model_ft,n_classes=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}